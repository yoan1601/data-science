{"cells":[{"source":"# Unsupervised Learning in Python\n\nRun the hidden code cell below to import the data used in this course.","metadata":{"id":"bA5ajAmk7XH6"},"id":"prostate-arizona","cell_type":"markdown"},{"source":"## Take Notes\n\nAdd notes about the concepts you've learned and code cells with code you want to keep.","metadata":{},"id":"a68ada23","cell_type":"markdown"},{"source":"**Clustering 2D points**\\\nFrom the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.\n\nYou are given the array points from the previous exercise, and also an array new_points.","metadata":{},"id":"ccb9c4ad","cell_type":"markdown"},{"source":"# Import KMeans\nfrom sklearn.cluster import KMeans\n\n# Create a KMeans instance with 3 clusters: model\nmodel = KMeans(n_clusters = 3)\n\n# Fit model to points\nmodel.fit(points)\n\n# Determine the cluster labels of new_points: labels\nlabels = model.predict(new_points)\n\n# Print cluster labels of new_points\nprint(labels)","metadata":{},"id":"2fb68042","cell_type":"code","execution_count":null,"outputs":[]},{"source":"**Inspect your clustering**\\\nLet's now inspect the clustering you performed in the previous exercise!\n\nA solution to the previous exercise has already run, so new_points is an array of points and labels is the array of their cluster labels.","metadata":{},"cell_type":"markdown","id":"1ecf7531-7756-424a-a9cb-ae4c38f3b713"},{"source":"# Import pyplot\nimport matplotlib.pyplot as plt \n\n# Assign the columns of new_points: xs and ys\nxs = new_points[:,0]\nys = new_points[:,1]\n\n# Make a scatter plot of xs and ys, using labels to define the colors\nplt.scatter(xs, ys, c=labels, alpha=0.5)\n\n# Assign the cluster centers: centroids\ncentroids = model.cluster_centers_\n\n# Assign the columns of centroids: centroids_x, centroids_y\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\n# Make a scatter plot of centroids_x and centroids_y\nplt.scatter(centroids_x,centroids_y,marker='D',s=50)\nplt.show()\n","metadata":{},"cell_type":"code","id":"09e3da00-4e58-4c77-8714-32f9a96a481a","outputs":[],"execution_count":null},{"source":"**How many clusters of grain?**\\\nIn the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array samples containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?\n\nKMeans and PyPlot (plt) have already been imported for you.\n\nThis dataset was sourced from the UCI Machine Learning Repository.","metadata":{},"cell_type":"markdown","id":"3d3e68ae-b6ca-4650-beb2-f9c96b3f7c06"},{"source":"ks = range(1, 6)\ninertias = []\n\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters = k)\n    \n    # Fit model to samples\n    model.fit(samples)\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n","metadata":{},"cell_type":"code","id":"60f24c5e-7c1b-41c0-92a1-bb0b7d7d2a4c","outputs":[],"execution_count":null},{"source":"**Evaluating the grain clustering**\\\nIn the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: \"Kama\", \"Rosa\" and \"Canadian\". In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.\n\nYou have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (pd) and KMeans have already been imported for you.","metadata":{},"cell_type":"markdown","id":"fadaf40c-0408-4bfb-af21-4f634661a472"},{"source":"# Create a KMeans model with 3 clusters: model\nmodel = KMeans(n_clusters = 3)\n\n# Use fit_predict to fit model and obtain cluster labels: labels\nlabels = model.fit_predict(samples)\n\n# Create a DataFrame with labels and varieties as columns: df\ndf = pd.DataFrame({'labels': labels, 'varieties': varieties})\n\n# Create crosstab: ct\nct = pd.crosstab(df['labels'], df['varieties'])\n\n# Display ct\nprint(ct)","metadata":{},"cell_type":"code","id":"36fa7466-88f6-4f27-828e-7bc2f8c158b3","outputs":[],"execution_count":null},{"source":"**Scaling fish data for clustering**\\\nYou are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.\n\nThese fish measurement data were sourced from the Journal of Statistics Education.","metadata":{},"cell_type":"markdown","id":"47753e64-63ba-43f4-ac21-5a265018ff18"},{"source":"# Perform the necessary imports\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create KMeans instance: kmeans\nkmeans = KMeans(n_clusters = 4)\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, kmeans)","metadata":{},"cell_type":"code","id":"919813a2-2b0a-47f5-9b76-6242a7608fea","outputs":[],"execution_count":null},{"source":"**Clustering the fish data**\\\nYou'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.\n\nAs before, samples is the 2D array of fish measurements. Your pipeline is available as pipeline, and the species of every fish sample is given by the list species.","metadata":{},"cell_type":"markdown","id":"e9db829c-e17d-4add-862d-f10238967c5f"},{"source":"# Import pandas\nimport pandas as pd\n\n# Fit the pipeline to samples\npipeline.fit(samples)\n\n# Calculate the cluster labels: labels\nlabels = pipeline.predict(samples)\n\n# Create a DataFrame with labels and species as columns: df\ndf = pd.DataFrame({'labels' : labels, 'species' : species})\n\n# Create crosstab: ct\nct = pd.crosstab(df['labels'], df['species'])\n\n# Display ct\nprint(ct)","metadata":{},"cell_type":"code","id":"26e4efb5-8a7c-4459-8612-553f333fcdbd","outputs":[],"execution_count":null},{"source":"**Clustering stocks using KMeans**\\\nIn this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.\n\nSome stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n\nNote that Normalizer() is different to StandardScaler(), which you used in the previous exercise. While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, Normalizer() rescales each sample - here, each company's stock price - independently of the other.\n\nKMeans and make_pipeline have already been imported for you.","metadata":{},"cell_type":"markdown","id":"211010a0-eaae-4897-9ff9-06584035b1a9"},{"source":"# Import Normalizer\nfrom sklearn.preprocessing import Normalizer\n\n# Create a normalizer: normalizer\nnormalizer = Normalizer()\n\n# Create a KMeans model with 10 clusters: kmeans\nkmeans = KMeans(n_clusters = 10)\n\n# Make a pipeline chaining normalizer and kmeans: pipeline\npipeline = make_pipeline(normalizer, kmeans)\n\n# Fit pipeline to the daily price movements\npipeline.fit(movements)\n","metadata":{},"cell_type":"code","id":"bcd84693-f723-4f06-b410-54fc72183ab3","outputs":[],"execution_count":null},{"source":"**Which stocks move together?**\\\nIn the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out.\n\nYour solution to the previous exercise has already been run. Recall that you constructed a Pipeline pipeline containing a KMeans model and fit it to the NumPy array movements of daily stock movements. In addition, a list companies of the company names is available.","metadata":{},"cell_type":"markdown","id":"d6eedbd9-c7f4-401a-a25a-1bca32e0ace6"},{"source":"# Import pandas\nimport pandas as pd\n\n# Predict the cluster labels: labels\nlabels = pipeline.predict(movements)\n\n# Create a DataFrame aligning labels and companies: df\ndf = pd.DataFrame({'labels': labels, 'companies': companies})\n\n# Display df sorted by cluster label\nprint(df.sort_values('labels'))\n","metadata":{},"cell_type":"code","id":"bc7dbec8-ac54-4437-85ef-18659eb661c6","outputs":[],"execution_count":null},{"source":"**Hierarchical clustering of the grain data**\\\nIn the video, you learned that the SciPy linkage() function performs hierarchical clustering on an array of samples. Use the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to visualize the result. A sample of the grain measurements is provided in the array samples, while the variety of each grain sample is given by the list varieties.","metadata":{},"cell_type":"markdown","id":"4413fafc-6b00-4c2d-9cb3-9d7313fb7363"},{"source":"# Perform the necessary imports\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt \n\n# Calculate the linkage: mergings\nmergings = linkage(samples, method = 'complete')\n\n# Plot the dendrogram, using varieties as labels\ndendrogram(mergings,\n           labels=varieties,\n           leaf_rotation=90,\n           leaf_font_size=6,\n)\nplt.show()\n\n","metadata":{},"cell_type":"code","id":"0873fa84-e805-4836-9168-5c5d068e9696","outputs":[],"execution_count":null},{"source":"![image](image.png)\n","metadata":{},"cell_type":"markdown","id":"81919ea5-638f-4252-ae79-e59738957671"},{"source":"**Hierarchies of stocks**\\\nIn chapter 1, you used k-means clustering to cluster companies according to their stock price movements. Now, you'll perform hierarchical clustering of the companies. You are given a NumPy array of price movements movements, where the rows correspond to companies, and a list of the company names companies. SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so you'll need to use the normalize() function from sklearn.preprocessing instead of Normalizer.\n\nlinkage and dendrogram have already been imported from scipy.cluster.hierarchy, and PyPlot has been imported as plt.","metadata":{},"cell_type":"markdown","id":"4e7ccfa9-448d-4cba-9041-01a2dbe413b1"},{"source":"**Different linkage, different hierarchical clustering!**\\\nIn the video, you saw a hierarchical clustering of the voting countries at the Eurovision song contest using 'complete' linkage. Now, perform a hierarchical clustering of the voting countries with 'single' linkage, and compare the resulting dendrogram with the one in the video. Different linkage, different hierarchical clustering!\n\nYou are given an array samples. Each row corresponds to a voting country, and each column corresponds to a performance that was voted for. The list country_names gives the name of each voting country. This dataset was obtained from Eurovision.","metadata":{},"cell_type":"markdown","id":"87a173e3-2b0d-4273-82cb-254b65d35a53"},{"source":"# Perform the necessary imports\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Calculate the linkage: mergings\nmergings = linkage(samples, method='single')\n\n# Plot the dendrogram\ndendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)\nplt.show()\n","metadata":{},"cell_type":"code","id":"afa547cd-b220-4627-9ad2-454f5756197a","outputs":[],"execution_count":null},{"source":"# Import normalize\nfrom sklearn.preprocessing import normalize \n\n# Normalize the movements: normalized_movements\nnormalized_movements = normalize(movements)\n\n# Calculate the linkage: mergings\nmergings = linkage(normalized_movements, method='complete')\n\n# Plot the dendrogram\ndendrogram(\n    mergings, \n    labels=companies,\n    leaf_rotation=90,\n    leaf_font_size=6\n)\nplt.show()","metadata":{},"cell_type":"code","id":"1445a00e-ded0-4f31-be0f-28c4e109f445","outputs":[],"execution_count":null},{"source":"**t-SNE visualization of grain dataset**\\\nIn the video, you saw t-SNE applied to the iris dataset. In this exercise, you'll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. You are given an array samples of grain samples and a list variety_numbers giving the variety number of each grain sample.","metadata":{},"cell_type":"markdown","id":"227f6a3b-6c66-4af9-ae3b-08891a6dc3e2"},{"source":"# Import TSNE\nfrom sklearn.manifold import TSNE \n\n# Create a TSNE instance: model\nmodel = TSNE(learning_rate=200)\n\n# Apply fit_transform to samples: tsne_features\ntsne_features = model.fit_transform(samples)\n\n# Select the 0th feature: xs\nxs = tsne_features[:,0]\n\n# Select the 1st feature: ys\nys = tsne_features[:,1]\n\n# Scatter plot, coloring by variety_numbers\nplt.scatter(xs, ys, c=variety_numbers)\nplt.show()","metadata":{},"cell_type":"code","id":"8aa5c532-436c-4bdb-93dc-c638b0d3851d","outputs":[],"execution_count":null},{"source":"**A t-SNE map of the stock market**\\\nt-SNE provides great visualizations when the individual samples can be labeled. In this exercise, you'll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives you a map of the stock market! The stock price movements for each company are available as the array normalized_movements (these have already been normalized for you). The list companies gives the name of each company. PyPlot (plt) has been imported for you.","metadata":{},"cell_type":"markdown","id":"2ec7da5f-6494-4f64-8ab0-a7c1722e98dc"},{"source":"# Import TSNE\nfrom sklearn.manifold import TSNE\n\n# Create a TSNE instance: model\nmodel = TSNE(learning_rate=50)\n\n# Apply fit_transform to normalized_movements: tsne_features\ntsne_features = model.fit_transform(normalized_movements)\n\n# Select the 0th feature: xs\nxs = tsne_features[:,0]\n\n# Select the 1th feature: ys\nys = tsne_features[:,1]\n\n# Scatter plot\nplt.scatter(xs, ys, alpha=0.5)\n\n# Annotate the points\nfor x, y, company in zip(xs, ys, companies):\n    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\nplt.show()\n","metadata":{},"cell_type":"code","id":"5545c386-e1c6-4093-b53c-cffdc0a6a326","outputs":[],"execution_count":null},{"source":"**Decorrelating the grain measurements with PCA**\\\nYou observed in the previous exercise that the width and length measurements of the grain are correlated. Now, you'll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation.","metadata":{},"cell_type":"markdown","id":"f79bc285-328b-4096-a4e1-b7067d9d5ab2"},{"source":"# Import PCA\nfrom sklearn.decomposition import PCA \n\n# Create PCA instance: model\nmodel = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model.fit_transform(grains)\n\n# Assign 0th column of pca_features: xs\nxs = pca_features[:,0]\n\n# Assign 1st column of pca_features: ys\nys = pca_features[:,1]\n\n# Scatter plot xs vs ys\nplt.scatter(xs, ys)\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation of xs and ys\ncorrelation, pvalue = pearsonr(xs, ys)\n\n# Display the correlation\nprint(correlation)","metadata":{},"cell_type":"code","id":"ea299369-214b-4c0c-b9a4-05f548da82d0","outputs":[],"execution_count":null},{"source":"**Variance of the PCA features**\\\nThe fish dataset is 6-dimensional. But what is its intrinsic dimension? Make a plot of the variances of the PCA features to find out. As before, samples is a 2D array, where each row represents a fish. You'll need to standardize the features first.","metadata":{},"cell_type":"markdown","id":"3b00801b-3d58-4a40-bbf1-676be99b7f9a"},{"source":"# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(samples)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()\n","metadata":{},"cell_type":"code","id":"1f18817d-5eb8-44ac-9928-4e7a00fc14a8","outputs":[],"execution_count":null},{"source":"**Dimension reduction of the fish measurements**\\\nIn a previous exercise, you saw that 2 was a reasonable choice for the \"intrinsic dimension\" of the fish measurements. Now use PCA for dimensionality reduction of the fish measurements, retaining only the 2 most important components.\n\nThe fish measurements have already been scaled for you, and are available as scaled_samples.","metadata":{},"cell_type":"markdown","id":"f6b2a7aa-933f-4834-afb5-53aecd333db7"},{"source":"# Import PCA\nfrom sklearn.decomposition import PCA \n\n# Create a PCA model with 2 components: pca\npca = PCA(n_components = 2)\n\n# Fit the PCA instance to the scaled samples\npca.fit(scaled_samples)\n\n# Transform the scaled samples: pca_features\npca_features = pca.transform(scaled_samples)\n\n# Print the shape of pca_features\nprint(pca_features.shape)","metadata":{},"cell_type":"code","id":"4407d076-0b2c-4633-81f7-8b3adcac72be","outputs":[],"execution_count":null},{"source":"**A tf-idf word-frequency array**\\\nIn this exercise, you'll create a tf-idf word frequency array for a toy collection of documents. For this, use the TfidfVectorizer from sklearn. It transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects.\n\nYou are given a list documents of toy documents about pets.","metadata":{},"cell_type":"markdown","id":"7bf93325-8687-47fb-94a2-8150869711cc"},{"source":"# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create a TfidfVectorizer: tfidf\ntfidf = TfidfVectorizer()\n\n# Apply fit_transform to document: csr_mat\ncsr_mat = tfidf.fit_transform(documents)\n\n# Print result of toarray() method\nprint(csr_mat.toarray())\n\n# Get the words: words\nwords = tfidf.get_feature_names()\n\n# Print words\nprint(words)","metadata":{},"cell_type":"code","id":"1efb8ac6-a05b-4b73-831b-beff3461a742","outputs":[],"execution_count":null},{"source":"**Clustering Wikipedia part I**\\\nYou saw in the video that TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. Combine your knowledge of TruncatedSVD and k-means to cluster some popular pages from Wikipedia. In this exercise, build the pipeline. In the next exercise, you'll apply it to the word-frequency array of some Wikipedia articles.\n\nCreate a Pipeline object consisting of a TruncatedSVD followed by KMeans. (This time, we've precomputed the word-frequency matrix for you, so there's no need for a TfidfVectorizer).\n\nThe Wikipedia dataset you will be working with was obtained from here.","metadata":{},"cell_type":"markdown","id":"e851f15b-1c91-4719-99d2-9277253fc871"},{"source":"# Perform the necessary imports\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\n\n# Create a TruncatedSVD instance: svd\nsvd = TruncatedSVD(n_components = 50)\n\n# Create a KMeans instance: kmeans\nkmeans = KMeans(n_clusters=6)\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(svd, kmeans)","metadata":{},"cell_type":"code","id":"ba3dcf88-2015-4dc5-a642-6264eb364050","outputs":[],"execution_count":null},{"source":"**Clustering Wikipedia part II**\\\nIt is now time to put your pipeline from the previous exercise to work! You are given an array articles of tf-idf word-frequencies of some popular Wikipedia articles, and a list titles of their titles. Use your pipeline to cluster the Wikipedia articles.\n\nA solution to the previous exercise has been pre-loaded for you, so a Pipeline pipeline chaining TruncatedSVD with KMeans is available.","metadata":{},"cell_type":"markdown","id":"a2cb394d-0d5d-4f2f-b17c-29b9e47e9cd4"},{"source":"# Import pandas\nimport pandas as pd \n\n# Fit the pipeline to articles\npipeline.fit(articles)\n\n# Calculate the cluster labels: labels\nlabels = pipeline.predict(articles)\n\n# Create a DataFrame aligning labels and titles: df\ndf = pd.DataFrame({'label': labels, 'article': titles})\n\n# Display df sorted by cluster label\nprint(df.sort_values('label'))","metadata":{},"cell_type":"code","id":"fc037855-41e9-4a8a-ba56-ea68bad3678a","outputs":[],"execution_count":null},{"source":"**NMF applied to Wikipedia articles**\\\nIn the video, you saw NMF applied to transform a toy word-frequency array. Now it's your turn to apply NMF, this time using the tf-idf word-frequency array of Wikipedia articles, given as a csr matrix articles. Here, fit the model and transform the articles. In the next exercise, you'll explore the result.","metadata":{},"cell_type":"markdown","id":"d930a8f3-d1f2-46c2-8b4e-5f94601b9c7b"},{"source":"# Import NMF\nfrom sklearn.decomposition import NMF \n\n# Create an NMF instance: model\nmodel = NMF(n_components = 6)\n\n# Fit the model to articles\nmodel.fit(articles)\n\n# Transform the articles: nmf_features\nnmf_features = model.transform(articles)\n\n# Print the NMF features\nprint(nmf_features.round(2))\n","metadata":{},"cell_type":"code","id":"6b5091c5-4777-4379-b207-667d72f6ca3a","outputs":[],"execution_count":null},{"source":"**NMF features of the Wikipedia articles**\\\nNow you will explore the NMF features you created in the previous exercise. A solution to the previous exercise has been pre-loaded, so the array nmf_features is available. Also available is a list titles giving the title of each Wikipedia article.\n\nWhen investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. In the next video, you'll see why: NMF components represent topics (for instance, acting!).","metadata":{},"cell_type":"markdown","id":"7d027877-9773-44f2-8e4f-bf179f174881"},{"source":"# Import pandas\nimport pandas as pd \n\n# Create a pandas DataFrame: df\ndf = pd.DataFrame(nmf_features, index=titles)\n\n# Print the row for 'Anne Hathaway'\nprint(df.loc[\"Anne Hathaway\"])\n\n# Print the row for 'Denzel Washington'\nprint(df.loc[\"Denzel Washington\"])\n\n\"\"\"\n<script.py> output:\n    0    0.004\n    1    0.000\n    2    0.000\n    3    0.576\n    4    0.000\n    5    0.000\n    Name: Anne Hathaway, dtype: float64\n    0    0.000\n    1    0.006\n    2    0.000\n    3    0.422\n    4    0.000\n    5    0.000\n    Name: Denzel Washington, dtype: float64\n\"\"\"","metadata":{},"cell_type":"code","id":"1d0a163c-bb74-4b05-84c9-1fc958dcca63","outputs":[],"execution_count":null},{"source":"**NMF learns topics of documents**\\\nIn the video, you learned when NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. Verify this for yourself for the NMF model that you built earlier using the Wikipedia articles. Previously, you saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. In this exercise, identify the topic of the corresponding NMF component.\n\nThe NMF model you built earlier is available as model, while words is a list of the words that label the columns of the word-frequency array.\n\nAfter you are done, take a moment to recognize the topic that the articles about Anne Hathaway and Denzel Washington have in common!","metadata":{},"cell_type":"markdown","id":"4b689234-521b-4016-893f-18b0a875e657"},{"source":"# Import pandas\nimport pandas as pd\n\n# Create a DataFrame: components_df\ncomponents_df = pd.DataFrame(model.components_, columns=words)\n\n# Print the shape of the DataFrame\nprint(components_df.shape)\n\n# Select row 3: component\ncomponent = components_df.iloc[3]\n\n# Print result of nlargest\nprint(component.nlargest())\n\n\"\"\"\n<script.py> output:\n    (6, 13125)\n    film       0.628\n    award      0.253\n    starred    0.245\n    role       0.211\n    actress    0.186\n    Name: 3, dtype: float64\n\"\"\"","metadata":{},"cell_type":"code","id":"b08672ee-b22b-4687-b943-b0026bd5dbf4","outputs":[],"execution_count":null},{"source":"**Explore the LED digits dataset**\\\nIn the following exercises, you'll use NMF to decompose grayscale images into their commonly occurring patterns. Firstly, explore the image dataset and see how it is encoded as an array. You are given 100 images as a 2D array samples, where each row represents a single 13x8 image. The images in your dataset are pictures of a LED digital display.","metadata":{},"cell_type":"markdown","id":"cf9ce079-ccc8-48d5-8a60-b64d2d035265"},{"source":"# Import pyplot\nfrom matplotlib import pyplot as plt\n\n# Select the 0th row: digit\ndigit = samples[0]\n\n# Print digit\nprint(digit)\n\n# Reshape digit to a 13x8 array: bitmap\nbitmap = digit.reshape(13,8)\n\n# Print bitmap\nprint(bitmap)\n\n# Use plt.imshow to display bitmap\nplt.imshow(bitmap, cmap='gray', interpolation='nearest')\nplt.colorbar()\nplt.show()\n","metadata":{},"cell_type":"code","id":"f346cf7f-2b50-452d-ad86-c7e669da59bb","outputs":[],"execution_count":null},{"source":"![Uploading image-2.png]()\n","metadata":{},"cell_type":"markdown","id":"2fc68a30-42d4-4aa3-a893-843d9e03e762"},{"source":"**NMF learns the parts of images**\\\nNow use what you've learned about NMF to decompose the digits dataset. You are again given the digit images as a 2D array samples. This time, you are also provided with a function show_as_image() that displays the image encoded by any 1D array:\n\n    def show_as_image(sample):\n        bitmap = sample.reshape((13, 8))\n        plt.figure()\n        plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n        plt.colorbar()\n        plt.show()\nAfter you are done, take a moment to look through the plots and notice how NMF has expressed the digit as a sum of the components!","metadata":{},"cell_type":"markdown","id":"7abee3d8-67fd-401b-b47d-68ce87337388"},{"source":"# Import NMF\nfrom sklearn.decomposition import NMF\n\n# Create an NMF model: model\nmodel = NMF(n_components = 7)\n\n# Apply fit_transform to samples: features\nfeatures = model.fit_transform(samples)\n\n# Call show_as_image on each component\nfor component in model.components_:\n    show_as_image(component)\n\n# Select the 0th row of features: digit_features\ndigit_features = features[0]\n\n# Print digit_features\nprint(digit_features)","metadata":{},"cell_type":"code","id":"eded1a08-422d-4e73-92b3-8d8a9ed0bce4","outputs":[],"execution_count":null},{"source":"**PCA doesn't learn parts**\\\nUnlike NMF, PCA doesn't learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. The images are available as a 2D array samples. Also available is a modified version of the show_as_image() function which colors a pixel red if the value is negative.\n\nAfter submitting the answer, notice that the components of PCA do not represent meaningful parts of images of LED digits!","metadata":{},"cell_type":"markdown","id":"2eb8658d-a6d5-496c-8cef-43ee7fbc26d1"},{"source":"# Import PCA\nfrom sklearn.decomposition import PCA\n\n# Create a PCA instance: model\nmodel = PCA(n_components = 7)\n\n# Apply fit_transform to samples: features\nfeatures = model.fit_transform(samples)\n\n# Call show_as_image on each component\nfor component in model.components_:\n    show_as_image(component)","metadata":{},"cell_type":"code","id":"3e17108c-4bfa-4097-a32f-5ce80746f48f","outputs":[],"execution_count":null},{"source":"**Which articles are similar to 'Cristiano Ronaldo'?**\\\nIn the video, you learned how to use NMF features and the cosine similarity to find similar articles. Apply this to your NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. The NMF features you obtained earlier are available as nmf_features, while titles is a list of the article titles.","metadata":{},"cell_type":"markdown","id":"47867e61-2ce6-4442-ab23-7af4750c2e64"},{"source":"# Perform the necessary imports\nimport pandas as pd\nfrom sklearn.preprocessing import normalize\n\n# Normalize the NMF features: norm_features\nnorm_features = normalize(nmf_features)\n\n# Create a DataFrame: df\ndf = pd.DataFrame(norm_features, index=titles)\n\n# Select the row corresponding to 'Cristiano Ronaldo': article\narticle = df.loc['Cristiano Ronaldo']\n\n# Compute the dot products: similarities\nsimilarities = df.dot(article)\n\n# Display those with the largest cosine similarity\nprint(similarities.nlargest())","metadata":{},"cell_type":"code","id":"ccb63619-6c65-44ba-974d-a5f1c6e1cf0c","outputs":[],"execution_count":null},{"source":"**Recommend musical artists part I**\\\nIn this exercise and the next, you'll use what you've learned about NMF to recommend popular music artists! You are given a sparse array artists whose rows correspond to artists and whose columns correspond to users. The entries give the number of times each artist was listened to by each user.\n\nIn this exercise, build a pipeline and transform the array into normalized NMF features. The first step in the pipeline, MaxAbsScaler, transforms the data so that all users have the same influence on the model, regardless of how many different artists they've listened to. In the next exercise, you'll use the resulting normalized NMF features for recommendation!","metadata":{},"cell_type":"markdown","id":"68fc2de6-c1f7-4900-a2a9-61adbd3966b8"},{"source":"# Perform the necessary imports\nfrom sklearn.decomposition import NMF\nfrom sklearn.preprocessing import Normalizer, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Create a MaxAbsScaler: scaler\nscaler = MaxAbsScaler()\n\n# Create an NMF model: nmf\nnmf = NMF(n_components = 20)\n\n# Create a Normalizer: normalizer\nnormalizer = Normalizer()\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(scaler, nmf, normalizer)\n\n# Apply fit_transform to artists: norm_features\nnorm_features = pipeline.fit_transform(artists)","metadata":{},"cell_type":"code","id":"3cd6afca-ee2a-4807-8085-1d5b23d3821c","outputs":[],"execution_count":null},{"source":"**Recommend musical artists part II**\\\nSuppose you were a big fan of Bruce Springsteen - which other musical artists might you like? Use your NMF features from the previous exercise and the cosine similarity to find similar musical artists. A solution to the previous exercise has been run, so norm_features is an array containing the normalized NMF features as rows. The names of the musical artists are available as the list artist_names.","metadata":{},"cell_type":"markdown","id":"f0f105eb-ea60-4d36-834f-3071b82c973d"},{"source":"# Import pandas\nimport pandas as pd \n\n# Create a DataFrame: df\ndf = pd.DataFrame(norm_features, index=artist_names)\n\n# Select row of 'Bruce Springsteen': artist\nartist = df.loc['Bruce Springsteen']\n\n# Compute cosine similarities: similarities\nsimilarities = df.dot(artist)\n\n# Display those with highest cosine similarity\nprint(similarities.nlargest())","metadata":{},"cell_type":"code","id":"dcfa348f-5947-4167-9189-a29acc3648d5","outputs":[],"execution_count":null},{"source":"Find in the hidden cell below some exercises to explore the data and practice your skills:","metadata":{},"id":"e54c1370","cell_type":"markdown"},{"source":"# Import the course packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport scipy.stats \n\n# Import the course datasets \ngrains = pd.read_csv('datasets/grains.csv')\nfish = pd.read_csv('datasets/fish.csv', header=None)\nwine = pd.read_csv('datasets/wine.csv')\neurovision = pd.read_csv('datasets/eurovision-2016.csv')\nstocks = pd.read_csv('datasets/company-stock-movements-2010-2015-incl.csv', index_col=0)\ndigits = pd.read_csv('datasets/lcd-digits.csv', header=None)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"2e25fdd8-4d84-45bc-80f0-949917e00a17","outputs":[],"execution_count":null},{"source":"## Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills!\n- You work for an agricultural research center. Your manager wants you to group seed varieties based on different measurements contained in the `grains` DataFrame. They also want to know how your clustering solution compares to the seed types listed in the dataset (the `variety_number` and `variety` columns). Try to use all of the relevant techniques you learned in Unsupervised Learning in Python!\n- In the `fish` DataFrame, each row represents an individual fish. Standardize the features and cluster the fish by their measurements. You can then compare your cluster labels with the actual fish species (first column).\n- In the `wine` DataFrame, there are three `class_labels` in this dataset. Transform the features to get the most accurate clustering.\n- In the `eurovision` DataFrame, perform hierarchical clustering of the voting countries using `complete` linkage and plot the resulting dendrogram.","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"collapsed":false},"id":"fiscal-syntax","cell_type":"markdown"}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}