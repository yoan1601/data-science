{"cells":[{"source":"# Supervised Learning with scikit-learn\nRun the hidden code cell below to import the data used in this course.","metadata":{"id":"bA5ajAmk7XH6"},"id":"prostate-arizona","cell_type":"markdown"},{"source":"# Importing pandas\nimport pandas as pd\n\n# Importing the course datasets \ndiabetes = pd.read_csv('datasets/diabetes_clean.csv')\nmusic = pd.read_csv('datasets/music_clean.csv')\nadvertising = pd.read_csv('datasets/advertising_and_sales_clean.csv')\ntelecom = pd.read_csv(\"datasets/telecom_churn_clean.csv\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"id":"2e25fdd8-4d84-45bc-80f0-949917e00a17","cell_type":"code","execution_count":null,"outputs":[]},{"source":"## Take Notes\n\nAdd notes about the concepts you've learned and code cells with code you want to keep.","metadata":{},"id":"0e7949e8","cell_type":"markdown"},{"source":"**Train/test split + computing accuracy**\n\nIt's time to practice splitting your data into training and test sets with the churn_df dataset!\n\nNumPy arrays have been created for you containing the features as X and the target variable as y.","metadata":{},"id":"18dd2afa","cell_type":"markdown"},{"source":"# Import the module\nfrom sklearn.model_selection import train_test_split\n\nX = churn_df.drop(\"churn\", axis=1).values\ny = churn_df[\"churn\"].values\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))","metadata":{},"id":"079c7628-464c-471b-9867-b684acc232b9","cell_type":"code","execution_count":null,"outputs":[]},{"source":"**Overfitting and underfitting**\\\nInterpreting model complexity is a great way to evaluate supervised learning performance. Your aim is to produce a model that can interpret the relationship between features and the target variable, as well as generalize well when exposed to new observations.\n\nThe training and test sets have been created from the churn_df dataset and preloaded as X_train, X_test, y_train, and y_test.\n\nIn addition, KNeighborsClassifier has been imported for you along with numpy as np.","metadata":{},"cell_type":"markdown","id":"5dec44df-2495-4e4b-b684-de25c8ed9f56"},{"source":"# Create neighbors\nneighbors = np.arange(1, 13)\ntrain_accuracies = {}\ntest_accuracies = {}\n\nfor neighbor in neighbors:\n  \n\t# Set up a KNN Classifier\n\tknn = KNeighborsClassifier(n_neighbors=neighbor)\n  \n\t# Fit the model\n\tknn.fit(X_train, y_train)\n  \n\t# Compute accuracy\n\ttrain_accuracies[neighbor] = knn.score(X_train, y_train)\n\ttest_accuracies[neighbor] = knn.score(X_test, y_test)\nprint(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)","metadata":{},"cell_type":"code","id":"37569d80-93b6-47bf-89b0-f2a37b10f04c","outputs":[],"execution_count":null},{"source":"**Visualizing model complexity**\n\nNow you have calculated the accuracy of the KNN model on the training and test sets using various values of n_neighbors, you can create a model complexity curve to visualize how performance changes as the model becomes less complex!\n\nThe variables neighbors, train_accuracies, and test_accuracies, which you generated in the previous exercise, have all been preloaded for you. You will plot the results to aid in finding the optimal number of neighbors for your model.","metadata":{},"cell_type":"markdown","id":"c091d61e-e7c2-4668-9f64-34183a092ab0"},{"source":"# Add a title\nplt.title(\"KNN: Varying Number of Neighbors\")\n\n# Plot training accuracies\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n\n# Plot test accuracies\nplt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\n\n# Display the plot\nplt.show()","metadata":{},"cell_type":"code","id":"1fd5f459-e0e9-4e3c-895b-868b29abe396","outputs":[],"execution_count":null},{"source":"**Fit and predict for regression**\\\nNow you have seen how linear regression works, your task is to create a multiple linear regression model using all of the features in the sales_df dataset, which has been preloaded for you. As a reminder, here are the first two rows:\n\n     tv        radio      social_media    sales\n1    13000.0   9237.76    2409.57         46677.90\n2    41000.0   15886.45   2913.41         150177.83\nYou will then use this model to predict sales based on the values of the test features.\n\nLinearRegression and train_test_split have been preloaded for you from their respective modules.","metadata":{},"cell_type":"markdown","id":"c81e57c6-1aaf-434c-817b-b55ab0a8e940"},{"source":"# Create X and y arrays\nX = sales_df.drop(\"sales\", axis=1).values\ny = sales_df[\"sales\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Instantiate the model\nreg = LinearRegression()\n\n# Fit the model to the data\nreg.fit(X_train, y_train)\n\n# Make predictions\ny_pred = reg.predict(X_test)\nprint(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))","metadata":{},"cell_type":"code","id":"9fb3ab82-648b-4c08-b6e3-5be1c0dd1641","outputs":[],"execution_count":null},{"source":"**Regression performance**\\\nNow you have fit a model, reg, using all features from sales_df, and made predictions of sales values, you can evaluate performance using some common regression metrics.\n\nThe variables X_train, X_test, y_train, y_test, and y_pred, along with the fitted model, reg, all from the last exercise, have been preloaded for you.\n\nYour task is to find out how well the features can explain the variance in the target values, along with assessing the model's ability to make predictions on unseen data.","metadata":{},"cell_type":"markdown","id":"28d78e59-56f1-4152-a200-804fac1ac868"},{"source":"# Import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\n\n# Compute R-squared\nr_squared = reg.score(X_test, y_test)\n\n# Compute RMSE\nrmse = mean_squared_error(y_test, y_pred, squared=False)\n\n# Print the metrics\nprint(\"R^2: {}\".format(r_squared))\nprint(\"RMSE: {}\".format(rmse))","metadata":{},"cell_type":"code","id":"bf5fa570-a687-4b27-abc6-086bfc98b7fe","outputs":[],"execution_count":null},{"source":"**Cross-validation for R-squared**\\\nCross-validation is a vital approach to evaluating a model. It maximizes the amount of data that is available to the model, as the model is not only trained but also tested on all of the available data.\n\nIn this exercise, you will build a linear regression model, then use 6-fold cross-validation to assess its accuracy for predicting sales using social media advertising expenditure. You will display the individual score for each of the six-folds.\n\nThe sales_df dataset has been split into y for the target variable, and X for the features, and preloaded for you. LinearRegression has been imported from sklearn.linear_model.","metadata":{},"cell_type":"markdown","id":"8cbeba75-5509-414d-839d-136b6f0b84f5"},{"source":"# Import the necessary modules\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# Create a KFold object\nkf = KFold(n_splits=6, shuffle=True, random_state=5)\n\nreg = LinearRegression()\n\n# Compute 6-fold cross-validation scores\ncv_scores = cross_val_score(reg, X, y, cv=kf)\n\n# Print scores\nprint(cv_scores)","metadata":{},"cell_type":"code","id":"370ae9e4-8cd1-496a-9314-79bcf995996a","outputs":[],"execution_count":null},{"source":"# Print the mean\nprint(np.mean(cv_results))\n\n# Print the standard deviation\nprint(np.std(cv_results))\n\n# Print the 95% confidence interval\nprint(np.quantile(cv_results, [0.025, 0.975]))","metadata":{},"cell_type":"code","id":"1743cfdc-a464-46de-b65d-f9c63b14df02","outputs":[],"execution_count":null},{"source":"**Regularized regression: Ridge**\\\nRidge regression performs regularization by computing the squared values of the model parameters multiplied by alpha and adding them to the loss function.\n\nIn this exercise, you will fit ridge regression models over a range of different alpha values, and print their \n scores. You will use all of the features in the sales_df dataset to predict \"sales\". The data has been split into X_train, X_test, y_train, y_test for you.\n\nA variable called alphas has been provided as a list containing different alpha values, which you will loop through to generate scores.","metadata":{},"cell_type":"markdown","id":"5d3afd79-08a8-4c8b-93b4-4ad555668e99"},{"source":"# Import Ridge\nfrom sklearn.linear_model import Ridge\nalphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\nridge_scores = []\nfor alpha in alphas:\n  \n  # Create a Ridge regression model\n  ridge = Ridge(alpha=alpha)\n  \n  # Fit the data\n  ridge.fit(X_train, y_train)\n  \n  # Obtain R-squared\n  score = ridge.score(X_test, y_test)\n  ridge_scores.append(score)\nprint(ridge_scores)","metadata":{},"cell_type":"code","id":"efb0790f-64b4-4820-bffd-eb73fc5a0b8c","outputs":[],"execution_count":null},{"source":"**Lasso regression for feature importance**\n\nIn the video, you saw how lasso regression can be used to identify important features in a dataset.\n\nIn this exercise, you will fit a lasso regression model to the sales_df data and plot the model's coefficients.\n\nThe feature and target variable arrays have been pre-loaded as X and y, along with sales_columns, which contains the dataset's feature names.","metadata":{},"cell_type":"markdown","id":"ba6dcc0e-560e-4951-a2b8-3f1906ffa07b"},{"source":"# Import Lasso\nfrom sklearn.linear_model import Lasso\n\n# Instantiate a lasso regression model\nlasso = Lasso(alpha = 0.3)\n\n# Fit the model to the data\nlasso.fit(X, y)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_\nprint(lasso_coef)\nplt.bar(sales_columns, lasso_coef)\nplt.xticks(rotation=45)\nplt.show()","metadata":{},"cell_type":"code","id":"2672778a-be6b-4e82-9384-fbb7243adf4c","outputs":[],"execution_count":null},{"source":"**Building a logistic regression model**\n\nIn this exercise, you will build a logistic regression model using all features in the diabetes_df dataset. The model will be used to predict the probability of individuals in the test set having a diabetes diagnosis.\n\nThe diabetes_df dataset has been split into X_train, X_test, y_train, and y_test, and preloaded for you.","metadata":{},"cell_type":"markdown","id":"aad6bd95-0b4f-4de1-aa58-e58c07b0f67a"},{"source":"# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate the model\nlogreg = LogisticRegression()\n\n# Fit the model\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities\ny_pred_probs = logreg.predict_proba(X_test)[:, 1]\n\nprint(y_pred_probs[:10])","metadata":{},"cell_type":"code","id":"e59ae407-33b6-47b4-8d01-0014164ba059","outputs":[],"execution_count":null},{"source":"**The ROC curve**\\\nNow you have built a logistic regression model for predicting diabetes status, you can plot the ROC curve to visualize how the true positive rate and false positive rate vary as the decision threshold changes.\n\nThe test labels, y_test, and the predicted probabilities of the test features belonging to the positive class, y_pred_probs, have been preloaded for you, along with matplotlib.pyplot as plt.\n\nYou will create a ROC curve and then interpret the results.","metadata":{},"cell_type":"markdown","id":"bba81391-0b4c-49a4-b6de-c5a25c6bf6a3"},{"source":"# Import roc_curve\nfrom sklearn.metrics import roc_curve\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n\nplt.plot([0, 1], [0, 1], 'k--')\n\n# Plot tpr against fpr\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Diabetes Prediction')\nplt.show()\n\n\"\"\"\nWell done! The ROC curve is above the dotted line, so the model performs better than randomly guessing the class of each observation.\n=> The model is much better than randomly guessing the class of each observation.\n\"\"\"","metadata":{},"cell_type":"code","id":"8fb261f7-a097-44d3-90ad-3e15f8debe3e","outputs":[],"execution_count":null},{"source":"**ROC AUC**\\\nThe ROC curve you plotted in the last exercise looked promising.\n\nNow you will compute the area under the ROC curve, along with the other classification metrics you have used previously.\n\nThe confusion_matrix and classification_report functions have been preloaded for you, along with the logreg model you previously built, plus X_train, X_test, y_train, y_test. Also, the model's predicted test set labels are stored as y_pred, and probabilities of test set observations belonging to the positive class stored as y_pred_probs.\n\nA knn model has also been created and the performance metrics printed in the console, so you can compare the roc_auc_score, confusion_matrix, and classification_report between the two models.","metadata":{},"cell_type":"markdown","id":"635b720e-b116-4a87-b701-c61a7bf2fdf4"},{"source":"# Import roc_auc_score\nfrom sklearn.metrics import roc_auc_score \n\n# Calculate roc_auc_score\nprint(roc_auc_score(y_test, y_pred_probs))\n\n# Calculate the confusion matrix\nprint(confusion_matrix(y_test, y_pred))\n\n# Calculate the classification report\nprint(classification_report(y_test, y_pred))\n\n\"\"\"\nDid you notice that logistic regression performs better than the KNN model across all the metrics you calculated? A ROC AUC score of 0.8002 means this model is 60% better than a chance model at correctly predicting labels! scikit-learn makes it easy to produce several classification metrics with only a few lines of code.\n\"\"\"","metadata":{},"cell_type":"code","id":"e35b400e-1c1a-4447-adca-74c43f776e89","outputs":[],"execution_count":null},{"source":"**Hyperparameter tuning with GridSearchCV**\\\nNow you have seen how to perform grid search hyperparameter tuning, you are going to build a lasso regression model with optimal hyperparameters to predict blood glucose levels using the features in the diabetes_df dataset.\n\nX_train, X_test, y_train, and y_test have been preloaded for you. A KFold() object has been created and stored for you as kf, along with a lasso regression model as lasso.","metadata":{},"cell_type":"markdown","id":"e766d6ff-4d3a-4578-bb6d-a159dcc6a463"},{"source":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the parameter grid\nparam_grid = {\"alpha\": np.linspace(0.00001, 1, 20)}\n\n# Instantiate lasso_cv\nlasso_cv = GridSearchCV(lasso, param_grid, cv=kf)\n\n# Fit to the training data\nlasso_cv.fit(X_train, y_train)\nprint(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\nprint(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))","metadata":{},"cell_type":"code","id":"e34c677e-ca8b-4c38-842b-e368d92e4aab","outputs":[],"execution_count":null},{"source":"**Hyperparameter tuning with RandomizedSearchCV**\\\nAs you saw, GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space. In this case, you can use RandomizedSearchCV, which tests a fixed number of hyperparameter settings from specified probability distributions.\n\nTraining and test sets from diabetes_df have been pre-loaded for you as X_train. X_test, y_train, and y_test, where the target is \"diabetes\". A logistic regression model has been created and stored as logreg, as well as a KFold variable stored as kf.\n\nYou will define a range of hyperparameters and use RandomizedSearchCV, which has been imported from sklearn.model_selection, to look for optimal hyperparameters from these options.","metadata":{},"cell_type":"markdown","id":"4681e4d5-eb36-473d-8dd9-f6ed9b677c7b"},{"source":"# Create the parameter space\nparams = {\"penalty\": [\"l1\", \"l2\"],\n         \"tol\": np.linspace(0.0001, 1.0, 50),\n         \"C\": np.linspace(0.1, 1.0, 50),\n         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n\n# Instantiate the RandomizedSearchCV object\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n\n# Fit the data to the model\nlogreg_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))","metadata":{},"cell_type":"code","id":"7a0901e4-41b6-475c-bc7a-e03b9f327506","outputs":[],"execution_count":null},{"source":"**Creating dummy variables**\\\nBeing able to include categorical features in the model building process can enhance performance as they may add information that contributes to prediction accuracy.\n\nThe music_df dataset has been preloaded for you, and its shape is printed. Also, pandas has been imported as pd.\n\nNow you will create a new DataFrame containing the original columns of music_df plus dummy variables from the \"genre\" column.","metadata":{},"cell_type":"markdown","id":"1d31e863-b359-4a26-868d-b687f81ef97d"},{"source":"# Create music_dummies\nmusic_dummies = pd.get_dummies(music_df, drop_first = True)\n\n# Print the new DataFrame's shape\nprint(\"Shape of music_dummies: {}\".format(music_dummies.shape))","metadata":{},"cell_type":"code","id":"6154d32a-3a90-465d-96c2-0c0728ca3725","outputs":[],"execution_count":null},{"source":"**Regression with categorical features**\\\nNow you have created music_dummies, containing binary features for each song's genre, it's time to build a ridge regression model to predict song popularity.\n\nmusic_dummies has been preloaded for you, along with Ridge, cross_val_score, numpy as np, and a KFold object stored as kf.\n\nThe model will be evaluated by calculating the average RMSE, but first, you will need to convert the scores for each fold to positive values and take their square root. This metric shows the average error of our model's predictions, so it can be compared against the standard deviation of the target value—\"popularity\".","metadata":{},"cell_type":"markdown","id":"4ff4f10b-6556-4a95-bb1a-a2d45c85839d"},{"source":"# Create X and y\nX = music_dummies.drop(\"popularity\", axis = 1)\ny = music_dummies[\"popularity\"]\n\n# Instantiate a ridge model\nridge = Ridge(alpha=0.2)\n\n# Perform cross-validation\nscores = cross_val_score(ridge, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n\n# Calculate RMSE\nrmse = np.sqrt(-scores)\nprint(\"Average RMSE: {}\".format(np.mean(rmse)))\nprint(\"Standard Deviation of the target array: {}\".format(np.std(y)))\n\n\"\"\"\nGreat work! An average RMSE of approximately 8.24 is lower than the standard deviation (14.02) of the target variable (song popularity), suggesting the model is reasonably accurate.\n\"\"\"","metadata":{},"cell_type":"code","id":"a5275e5b-8d4b-4769-a34f-33392f66ee47","outputs":[],"execution_count":null},{"source":"**Dropping missing data**\\\nOver the next three exercises, you are going to tidy the music_df dataset. You will create a pipeline to impute missing values and build a KNN classifier model, then use it to predict whether a song is of the \"Rock\" genre.\n\nIn this exercise specifically, you will drop missing values accounting for less than 5% of the dataset, and convert the \"genre\" column into a binary feature.","metadata":{},"cell_type":"markdown","id":"6681dae9-2bc4-4d5d-82c9-0750ef3a7000"},{"source":"# Print missing values for each column\nprint(music_df.isna().sum().sort_values())\n\n# Remove values where less than 5% are missing\nmusic_df = music_df.dropna(subset=[\"genre\", \"popularity\", \"loudness\", \"liveness\", \"tempo\"])\n\n# Convert genre to a binary feature\nmusic_df[\"genre\"] = np.where(music_df[\"genre\"] == \"Rock\", 1, 0)\n\nprint(music_df.isna().sum().sort_values())\nprint(\"Shape of the `music_df`: {}\".format(music_df.shape))","metadata":{},"cell_type":"code","id":"416df55f-1a17-4298-b1ec-c9720c2166c4","outputs":[],"execution_count":null},{"source":"**Pipeline for song genre prediction: I**\\\nNow it's time to build a pipeline. It will contain steps to impute missing values using the mean for each feature and build a KNN model for the classification of song genre.\n\nThe modified music_df dataset that you created in the previous exercise has been preloaded for you, along with KNeighborsClassifier and train_test_split.","metadata":{},"cell_type":"markdown","id":"33bc130c-e7d6-4a70-ad3f-ab6789ea8c54"},{"source":"# Import modules\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n# Instantiate an imputer\nimputer = SimpleImputer()\n\n# Instantiate a knn model\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Build steps for the pipeline\nsteps = [(\"imputer\", imputer), \n         (\"knn\", knn)]","metadata":{},"cell_type":"code","id":"9d3cf26a-4f6f-4d91-af81-12f54787e677","outputs":[],"execution_count":null},{"source":"**Pipeline for song genre prediction: II**\\\nHaving set up the steps of the pipeline in the previous exercise, you will now use it on the music_df dataset to classify the genre of songs. What makes pipelines so incredibly useful is the simple interface that they provide.\n\nX_train, X_test, y_train, and y_test have been preloaded for you, and confusion_matrix has been imported from sklearn.metrics.\n\n","metadata":{},"cell_type":"markdown","id":"7f95ce64-00d5-4989-ab60-16fc97045d63"},{"source":"steps = [(\"imputer\", imp_mean),\n        (\"knn\", knn)]\n\n# Create the pipeline\npipeline = Pipeline(steps)\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Print the confusion matrix\nprint(confusion_matrix(y_test, y_pred))","metadata":{},"cell_type":"code","id":"ecb2d21c-d9e5-40e0-9047-a1464401f5fa","outputs":[],"execution_count":null},{"source":"**Centering and scaling for regression**\\\nNow you have seen the benefits of scaling your data, you will use a pipeline to preprocess the music_df features and build a lasso regression model to predict a song's loudness.\n\nX_train, X_test, y_train, and y_test have been created from the music_df dataset, where the target is \"loudness\" and the features are all other columns in the dataset. Lasso and Pipeline have also been imported for you.\n\nNote that \"genre\" has been converted to a binary feature where 1 indicates a rock song, and 0 represents other genres.","metadata":{},"cell_type":"markdown","id":"c79b20ae-db08-4cee-8971-bd54f6377ba4"},{"source":"# Import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Create pipeline steps\nsteps = [(\"scaler\", StandardScaler()),\n         (\"lasso\", Lasso(alpha=0.5))]\n\n# Instantiate the pipeline\npipeline = Pipeline(steps)\npipeline.fit(X_train, y_train)\n\n# Calculate and print R-squared\nprint(pipeline.score(X_test, y_test))","metadata":{},"cell_type":"code","id":"27b031e7-feec-4637-b757-550a8be7d8e9","outputs":[],"execution_count":null},{"source":"**Centering and scaling for classification**\\\nNow you will bring together scaling and model building into a pipeline for cross-validation.\n\nYour task is to build a pipeline to scale features in the music_df dataset and perform grid search cross-validation using a logistic regression model with different values for the hyperparameter C. The target variable here is \"genre\", which contains binary values for rock as 1 and any other genre as 0.\n\nStandardScaler, LogisticRegression, and GridSearchCV have all been imported for you.","metadata":{},"cell_type":"markdown","id":"1ff2552d-413f-4073-a166-8291c813c515"},{"source":"# Build the steps\nsteps = [(\"scaler\", StandardScaler()),\n         (\"logreg\", LogisticRegression())]\npipeline = Pipeline(steps)\n\n# Create the parameter space\nparameters = {\"logreg__C\": np.linspace(0.001, 1.0, 20)}\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=21)\n\n# Instantiate the grid search object\ncv = GridSearchCV(pipeline, param_grid=parameters)\n\n# Fit to the training data\ncv.fit(X_train, y_train)\nprint(cv.best_score_, \"\\n\", cv.best_params_)","metadata":{},"cell_type":"code","id":"cd775b1e-47f3-4898-a468-6304b830daaa","outputs":[],"execution_count":null},{"source":"**Visualizing regression model performance**\\\nNow you have seen how to evaluate multiple models out of the box, you will build three regression models to predict a song's \"energy\" levels.\n\nThe music_df dataset has had dummy variables for \"genre\" added. Also, feature and target arrays have been created, and these have been split into X_train, X_test, y_train, and y_test.\n\nThe following have been imported for you: LinearRegression, Ridge, Lasso, cross_val_score, and KFold.","metadata":{},"cell_type":"markdown","id":"8362813a-cdf9-4947-87ac-ee1a794dc453"},{"source":"models = {\"Linear Regression\": LinearRegression(), \"Ridge\": Ridge(alpha=0.1), \"Lasso\": Lasso(alpha=0.1)}\nresults = []\n\n# Loop through the models' values\nfor model in models.values():\n  kf = KFold(n_splits=6, random_state=42, shuffle=True)\n  \n  # Perform cross-validation\n  cv_scores = cross_val_score(model, X_train, y_train, cv=kf)\n  \n  # Append the results\n  results.append(cv_scores)\n\n# Create a box plot of the results\nplt.boxplot(results, labels=models.keys())\nplt.show()","metadata":{},"cell_type":"code","id":"efd2bf28-afc8-4053-8281-d8f5f2452d10","outputs":[],"execution_count":null},{"source":"**Predicting on the test set**\\\nIn the last exercise, linear regression and ridge appeared to produce similar results. It would be appropriate to select either of those models; however, you can check predictive performance on the test set to see if either one can outperform the other.\n\nYou will use root mean squared error (RMSE) as the metric. The dictionary models, containing the names and instances of the two models, has been preloaded for you along with the training and target arrays X_train_scaled, X_test_scaled, y_train, and y_test.","metadata":{},"cell_type":"markdown","id":"3cc1a47c-671c-484c-a389-6f8fe7c20caf"},{"source":"# Import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\n\nfor name, model in models.items():\n  \n  # Fit the model to the training data\n  model.fit(X_train_scaled, y_train)\n  \n  # Make predictions on the test set\n  y_pred = model.predict(X_test_scaled)\n  \n  # Calculate the test_rmse\n  test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n  print(\"{} Test Set RMSE: {}\".format(name, test_rmse))","metadata":{},"cell_type":"code","id":"c71aa3f9-f28e-4338-bff3-0c5ef6a4c1e9","outputs":[],"execution_count":null},{"source":"**Visualizing classification model performance**\\\nIn this exercise, you will be solving a classification problem where the \"popularity\" column in the music_df dataset has been converted to binary values, with 1 representing popularity more than or equal to the median for the \"popularity\" column, and 0 indicating popularity below the median.\n\nYour task is to build and visualize the results of three different models to classify whether a song is popular or not.\n\nThe data has been split, scaled, and preloaded for you as X_train_scaled, X_test_scaled, y_train, and y_test. Additionally, KNeighborsClassifier, DecisionTreeClassifier, and LogisticRegression have been imported.","metadata":{},"cell_type":"markdown","id":"4581a60c-e8f7-4576-a1bd-ff3267524063"},{"source":"# Create models dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"Decision Tree Classifier\": DecisionTreeClassifier()}\nresults = []\n\n# Loop through the models' values\nfor model in models.values():\n  \n  # Instantiate a KFold object\n  kf = KFold(n_splits=6, random_state=12, shuffle=True)\n  \n  # Perform cross-validation\n  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n  results.append(cv_results)\nplt.boxplot(results, labels=models.keys())\nplt.show()","metadata":{},"cell_type":"code","id":"77c9fc6d-a167-43c5-9202-d7d903093ccb","outputs":[],"execution_count":null},{"source":"**Pipeline for predicting song popularity**\\\nFor the final exercise, you will build a pipeline to impute missing values, scale features, and perform hyperparameter tuning of a logistic regression model. The aim is to find the best parameters and accuracy when predicting song genre!\n\nAll the models and objects required to build the pipeline have been preloaded for you.","metadata":{},"cell_type":"markdown","id":"e38ad4c0-e838-46fa-b796-1f171dedad7e"},{"source":"# Create steps\nsteps = [(\"imp_mean\", SimpleImputer()), \n         (\"scaler\", StandardScaler()), \n         (\"logreg\", LogisticRegression())]\n\n# Set up pipeline\npipeline = Pipeline(steps)\nparams = {\"logreg__solver\": [\"newton-cg\", \"saga\", \"lbfgs\"],\n         \"logreg__C\": np.linspace(0.001, 1.0, 10)}\n\n# Create the GridSearchCV object\ntuning = GridSearchCV(pipeline, param_grid=params)\ntuning.fit(X_train, y_train)\ny_pred = tuning.predict(X_test)\n\n# Compute and print performance\nprint(\"Tuned Logistic Regression Parameters: {}, Accuracy: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))","metadata":{},"cell_type":"code","id":"85170b45-3233-4f4b-9495-d032903428e7","outputs":[],"execution_count":null}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}